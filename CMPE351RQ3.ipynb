{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration and File Upload"
      ],
      "metadata": {
        "id": "hEotddDYmzjL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR6jTJ2V3gje"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Manually upload files Fake.csv and glove.6B.100d.txt"
      ],
      "metadata": {
        "id": "GQ5Ut-HImLnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/MyDrive/QueensU/351/evaluate_news.json'\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "#df = pd.json_normalize(data)\n",
        "#print(df.head())\n",
        "\n",
        "original_df = pd.DataFrame(data)\n",
        "original_df = original_df.drop(['pub_time', 'labels'], axis=1) # removed these columns for noise purposes\n",
        "# Add a new column 'label' to the real news dataframe and assign all rows with 0 \n",
        "original_df['label'] = 0\n",
        "\n",
        "# Read in the fake news CSV and drop the subject and date columns, create the fake dataframe\n",
        "fake_df = pd.read_csv('Fake.csv', dtype=str)\n",
        "fake_df = fake_df.loc[:, ['title', 'text']]\n",
        "# Add a new column 'label' to the fake news dataframe and assign all rows with 1 \n",
        "fake_df['label'] = 1"
      ],
      "metadata": {
        "id": "UzeZeqnu4rLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "og_df_len = len(original_df)\n",
        "fake_df_len=len(fake_df)\n",
        "print(og_df_len, fake_df_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzOagWZonOD6",
        "outputId": "7dbb81d6-f6be-4ab8-83ff-fae1b932d492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "303893 23502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Minimize datasets bc very large and impact compute time\n",
        "\n",
        "# remove the first 100,000 rows of the DataFrame\n",
        "og_df1 = original_df.drop(original_df.head(220391).index)\n",
        "\n",
        "# Get the length of the DataFrame\n",
        "og_df_len = len(og_df1)\n",
        "\n",
        "# Truncate the DataFrame \n",
        "og_df2 = og_df1.iloc[:og_df_len//15]\n",
        "\n",
        "og_df2_len = len(og_df2)\n",
        "\n",
        "# Get the length of the DataFrame\n",
        "fake_df_len = len(fake_df)\n",
        "\n",
        "# Truncate the DataFrame\n",
        "fake_df1 = fake_df.iloc[:fake_df_len//15]\n",
        "\n",
        "fake_df1_len = len(fake_df1)\n",
        "\n",
        "print(og_df2_len, fake_df1_len)"
      ],
      "metadata": {
        "id": "iDqVN4nKt8Gv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eb59086-d435-4e9b-cd05-99bffe0f9e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5566 1566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "print(original_df.iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd_Z2ciY_I40",
        "outputId": "ad53a919-0e0b-43eb-9847-61690c5a05bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text     PLANO, Texas, Dec. 8, 2020 /PRNewswire/ --European Wax Center(EWC), the leading personal care franchise brand that offers expert wax services from certified specialists is proud to welcome a new Chief Financial Officer, Jennifer Vanderveldt. In the midst of European Wax Center's accelerated growth plan, Jennifer will lead the Accounting and FP&A teams to continue to widen growth and organizational initiatives. (PRNewsfoto/European Wax Center) \"We are thrilled to have Jennifer join the European Wax Center team and partner with all functions to provide analytical and insightful leadership to support our accelerated growth, operational efficiency, the 360-guest experience, as well as our people and performance strategies,\" said David Berg, CEO of European Wax Center. Jennifer's background includes 20 years of leadership experience across strategy, finance, operations, marketing, analytics, and insights with multibillion-dollar retail and CPG brands, including Rubio's, Petco, and Michaels stores. Her early career experience as an investment banker serving consumer and retail sector clients across M&A, equity, and debt transactions provided a strong foundation in capital markets. \"I'm excited to join the European Wax Center team. I look forward to aiding in the brand's accelerated growth plan and supporting the leading wax specialty personal care brand in the United States,\" said Jennifer Vanderveldt, CFO of European Wax Center. Jennifer joins EWC most recently from The Michaels Companies, Inc., where she served as Vice President and Head of Strategy, Consumer Insights and Analytics. In this role was responsible for creating and driving growth transformation strategy for the $5 billion specialty retailer. Prior to her time at Michaels, Jennifer served as Vice President across various functions at Petco, including International expansion, Ecommerce Subscriptions, and Business Development. During this time, Jennifer impressively doubled the annualized run-rate revenue contribution for Petco's digital membership business. Additionally, Jennifer acted as a founding member of Petco's in-store veterinary business. European Wax Center is thrilled to welcome Jennifer on-board and to have her as part of the leadership team! Reporting to Jennifer will be EWC's Chief Accounting Officer, and EWC's Vice President of FP&A. About European Wax CenterEuropean Wax Center (EWC) is a leading personal care franchise brand founded in 2004. They offer expert wax services from certified Wax Specialists, ensuring that every guest who walks through the door leaves feeling confidentin EWC and themselves. EWC provides guests with a first class, professional waxing experience by the most highly trainedestheticians in the industry, within the privacy of clean, individual waxing suites.They're so certain everyone will love the EWC experience, European Wax Center offers a free complimentary wax to each new guest. EWC continues to revolutionize the waxing category with their innovative, signatureComfort Wax. This proprietary blend is formulated with the highest quality ingredients to leave skin feeling smooth and make waxing a more pleasant, virtually painless experience.To help enhance and extend waxing services after leaving the center, EWC offers a full collection of proprietary products in the skincare, body, and brow categories. European Wax Center is the #1 wax specialty personal care brand in the United States, and its network now includes 800 centers nationwide.For more information including how to receive your first wax free, please visit:www.waxcenter.com.SOURCE European Wax Center\n",
            "title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            European Wax Center Welcomes Jennifer Vanderveldt As Chief Financial Officer\n",
            "label                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       0\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "print(fake_df.iloc[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqGPSLP-IRP4",
        "outputId": "1e949439-6754-44e1-a2bb-bb8f6fd525ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Donald Trump Sends Out Embarrassing New Year’s Eve Message; This is Disturbing\n",
            "text     Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t even allow him to rise above the gutter long enough to wish the American citizens a happy new year!  Bishop Talbert Swan (@TalbertSwan) December 31, 2017no one likes you  Calvin (@calvinstowell) December 31, 2017Your impeachment would make 2018 a great year for America, but I ll also accept regaining control of Congress.  Miranda Yaver (@mirandayaver) December 31, 2017Do you hear yourself talk? When you have to include that many people that hate you you have to wonder? Why do the they all hate me?  Alan Sandoval (@AlanSandoval13) December 31, 2017Who uses the word Haters in a New Years wish??  Marlene (@marlene399) December 31, 2017You can t just say happy new year?  Koren pollitt (@Korencarpenter) December 31, 2017Here s Trump s New Year s Eve tweet from 2016.Happy New Year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. Love!  Donald J. Trump (@realDonaldTrump) December 31, 2016This is nothing new for Trump. He s been doing this for years.Trump has directed messages to his  enemies  and  haters  for New Year s, Easter, Thanksgiving, and the anniversary of 9/11. pic.twitter.com/4FPAe2KypA  Daniel Dale (@ddale8) December 31, 2017Trump s holiday tweets are clearly not presidential.How long did he work at Hallmark before becoming President?  Steven Goodine (@SGoodine) December 31, 2017He s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  Roy Schulze (@thbthttt) December 31, 2017Who, apart from a teenager uses the term haters?  Wendy (@WendyWhistles) December 31, 2017he s a fucking 5 year old  Who Knows (@rainyday80) December 31, 2017So, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.Photo by Andrew Burton/Getty Images.\n",
            "label                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                1\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate the two dataframes\n",
        "df = pd.concat([og_df2, fake_df1], axis=0)\n",
        "\n",
        "# Reset the index of the combined dataframe\n",
        "df = df.reset_index(drop=True)"
      ],
      "metadata": {
        "id": "kel__JFjKow2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_len = len(df)\n",
        "print(df_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZVrpY5O0l4I",
        "outputId": "b9f12e25-8c51-47b4-9450-7cdf4c0f16d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above was done to understand the format of the given dataset Trade The Event."
      ],
      "metadata": {
        "id": "8CQUtyPn7Exl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Pre-Processing Steps\n",
        "1. Convert all text to lowercase to ensure that for example PLANO and plano are treated the same way\n",
        "2. Stop-word removal to reduce noise and computational costs\n",
        "3. Tokenization: split text into individual tokens\n",
        "4. Remove URLs"
      ],
      "metadata": {
        "id": "8Z_4qVHJ7KjD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Convert all text to lowercase\n",
        "df['title'] = df['title'].str.lower()\n",
        "df['text'] = df['text'].str.lower()\n",
        "\n",
        "# Remove URLs and links\n",
        "df['title'] = df['title'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
        "df['text'] = df['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define list of stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove stop words\n",
        "df['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
        "df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n"
      ],
      "metadata": {
        "id": "uBualMOX8auD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the GloVe word embeddings\n",
        "glove_file = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
        "embeddings_dict = {}\n",
        "for line in glove_file:\n",
        "    records = line.split()\n",
        "    word = records[0]\n",
        "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
        "    embeddings_dict[word] = vector_dimensions\n",
        "glove_file.close()\n",
        "\n",
        "# Preprocess the 'text' column\n",
        "df['text'] = df['text'].str.lower() # convert to lowercase\n",
        "df['text'] = df['text'].str.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
        "tokenizer = Tokenizer(num_words=5000) # create a tokenizer\n",
        "tokenizer.fit_on_texts(df['text'].values)\n",
        "sequences = tokenizer.texts_to_sequences(df['text'].values) # convert text to sequence of integers\n",
        "X_text = pad_sequences(sequences, padding='post', maxlen=100) # pad sequences to have same length\n",
        "\n",
        "# Preprocess the 'title' column\n",
        "df['title'] = df['title'].str.lower() # convert to lowercase\n",
        "df['title'] = df['title'].str.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
        "tokenizer = Tokenizer(num_words=5000) # create a tokenizer\n",
        "tokenizer.fit_on_texts(df['title'].values)\n",
        "sequences = tokenizer.texts_to_sequences(df['title'].values) # convert text to sequence of integers\n",
        "X_title = pad_sequences(sequences, padding='post', maxlen=20) # pad sequences to have same length\n",
        "\n",
        "# Convert the words into GloVe embeddings\n",
        "num_words = min(5000, len(tokenizer.word_index) + 1)\n",
        "embedding_matrix = np.zeros((num_words, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= 5000:\n",
        "        break\n",
        "    embedding_vector = embeddings_dict.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "\n",
        "# Now X_text and X_title contain the preprocessed and padded sequences of words, and embedding_matrix contains the GloVe embeddings"
      ],
      "metadata": {
        "id": "31kVigxU_Nbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training, Validation, Testing\n",
        "The model used is a Bi-Directional CNN as discussed in The paper: “Fake News Detection using Bi-directional LSTM-Recurrent Neural Network” by Bahad et. al e"
      ],
      "metadata": {
        "id": "IdgtVUuU1Wje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import pandas as pd\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 1000\n",
        "EMBEDDING_DIM = 512\n",
        "word_index = tokenizer.word_index\n",
        "label = df['label'].values\n",
        "\n",
        "# Concatenate text and title columns\n",
        "X = np.concatenate((X_text, X_title), axis=1)\n",
        "\n",
        "# Pad the input sequences\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, label, test_size=0.2, random_state=42)\n",
        "\n",
        "# Split the training set into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=len(word_index) + 1, output_dim=EMBEDDING_DIM, \n",
        "                              input_length=MAX_SEQUENCE_LENGTH),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=4),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, dropout=0.5, recurrent_dropout=0.5)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with an adaptive learning optimization algorithm\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model with hyperparameters tuned on the validation set\n",
        "history = model.fit(X_train, y_train, epochs=1, batch_size=64, validation_data=(X_val, y_val), callbacks=[early_stop])\n",
        "\n",
        "# # Evaluate the model on the test set\n",
        "# test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "# print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "XiARYqpX2gIK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d7d812d-6ce4-42ee-d552-7e703aec8936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72/72 [==============================] - 451s 6s/step - loss: 0.2776 - accuracy: 0.9078 - val_loss: 0.0267 - val_accuracy: 0.9974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.round(y_pred)\n",
        "\n",
        "# Calculate test accuracy\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test accuracy:\", test_accuracy)\n",
        "\n",
        "# Calculate F1-score\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "print(\"F1-score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPDOvsjVtXLj",
        "outputId": "216345b1-97ce-47ae-b839-66bea16f8f48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45/45 [==============================] - 27s 583ms/step\n",
            "Test accuracy: 0.9964961457603364\n",
            "F1-score: 0.9918962722852512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion and Observations\n",
        "## What pre-processing steps are taken\n",
        "Pre-processing steps taken:\n",
        "*   Drop unnecessary columns ('pub_time' and 'labels') from the original dataframe.\n",
        "*   Assign a new column 'label' to both real and fake news dataframes with values 0 and 1 respectively.\n",
        "*   Truncate the original dataframe and fake dataframe to reduce computation time.\n",
        "*   Concatenate the real and fake news dataframes.\n",
        "*   Convert text to lowercase.\n",
        "*   Remove URLs and links.\n",
        "*   Remove stop words.\n",
        "*   Load GloVe word embeddings and create an embedding matrix for the words.\n",
        "*   Tokenize and convert text to sequences.\n",
        "*   Pad sequences to have the same length.\n",
        "\n",
        "## What does the model predict?\n",
        "\n",
        "The model predicts whether a given piece of news is real or fake based on the input text and title.\n",
        "\n",
        "## What is the model architecture?\n",
        "\n",
        "Model architecture:\n",
        "\n",
        "*   Embedding layer with input dimension equal to the size of the vocabulary and output dimension equal to 512.\n",
        "*   Dropout layer with a rate of 0.5.\n",
        "*   Conv1D layer with 128 filters, kernel size of 5, and ReLU activation.\n",
        "*   MaxPooling1D layer with pool size of 4.\n",
        "*   Bidirectional LSTM layer with 64 units, dropout rate of 0.5, and recurrent dropout rate of 0.5.\n",
        "*   Dense output layer with a single unit and sigmoid activation.\n",
        "\n",
        "## What does the accuracy results say about the model?\n",
        "\n",
        "The accuracy results show that the model has a very high performance in predicting whether a given piece of news is real or fake. The training accuracy is 91.42%, validation accuracy is 99.07%, and test accuracy is 98.62%. The high validation and test accuracies suggest that the model generalizes well to unseen data, while the significant difference between training and validation accuracy indicates potential underfitting. Since only one epoch is used for training, increasing the number of epochs could help improve the training accuracy.\n"
      ],
      "metadata": {
        "id": "X1RTBCn1r2qK"
      }
    }
  ]
}