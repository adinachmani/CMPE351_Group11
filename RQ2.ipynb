{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7b31767",
   "metadata": {},
   "source": [
    "# Import Libraries, Read in .json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abb9208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from datetime import date\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "df = pd.read_json('evaluate_news.json')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "811d207b",
   "metadata": {},
   "source": [
    "# Preprocessing, Developing New Columns\n",
    "Get the labels, find the change from start price day 1 to end of day 3. \n",
    "Drop unavailable ticker data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e360d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Get the labels, normalize them, and concatenate to the dataframe. Then, add a new column for the overall baseline change from start day 1 to end of day 3.\n",
    "labels = df[\"labels\"]\n",
    "labels = pd.json_normalize(labels)\n",
    "data = pd.concat([df,labels],axis=1)\n",
    "data[\"baselinePctChng3\"] = ((data[\"end_price_3day\"] - data[\"start_price_open\"]) / data[\"start_price_open\"])*100\n",
    "data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17f85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb261e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91592c7e",
   "metadata": {},
   "source": [
    "# Develop the Corpus\n",
    "Entries: 57,031 total with positive, 48,955 total entries with negative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8facdc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a167077",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataShuffled = data.sample(frac=1)\n",
    "dataPos = dataShuffled[dataShuffled[\"baselinePctChng3\"] > 0]\n",
    "dataNeg = dataShuffled[dataShuffled[\"baselinePctChng3\"] < 0]\n",
    "# print(len(dataPos))\n",
    "# print(len(dataNeg))\n",
    "\n",
    "posX = dataPos.sample(frac = 10000/57031, random_state=1)\n",
    "negX = dataNeg.sample(frac = 10000/48955,random_state=1)\n",
    "# print(posX.shape)\n",
    "# print(negX.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e508e755",
   "metadata": {},
   "source": [
    "# Determine most important words using TF-IDF w/transformer, TF-IDF w/vectorizer, and purely based on frequency (no model, just math)\n",
    "\n",
    "typically, If you need the term frequency (term count) vectors for different tasks, use transformer. If you need to compute tf-idf scores on documents within your “training” dataset, use vectorizer.\n",
    "\n",
    "Note: results got the same 'top' words."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "970d7e5b",
   "metadata": {},
   "source": [
    "### Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714b755",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF with transformer\n",
    "\n",
    "sentencesPositive = []\n",
    "wordsPositive = []\n",
    "\n",
    "for index, row in posX.iterrows():\n",
    "    sentence = row['title']\n",
    "    sentencesPositive.append(sentence)\n",
    "\n",
    "print(len(sentencesPositive))\n",
    "\n",
    "countVector = CountVectorizer()\n",
    "wordCountVector = countVector.fit_transform(sentencesPositive)\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(wordCountVector)\n",
    "\n",
    "matrix = countVector.transform(sentencesPositive)\n",
    "tf_idf_vector=tfidf_transformer.transform(matrix)\n",
    "\n",
    "feature_names = countVector.get_feature_names_out() \n",
    "first_document_vector=tf_idf_vector[0] \n",
    "\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a68b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "posDict = dict(zip(df.index, df['tfidf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb2bacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF with vectorizer\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "# settings that you use for count vectorizer will go here \n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
    "\n",
    "#transform, send in\n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(sentencesPositive)\n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
    "\n",
    "#place in dataframe, sort by highest weight.\n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names_out(), columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82b2b6ee",
   "metadata": {},
   "source": [
    "### Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a92692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF with transformer\n",
    "sentencesNegative = []\n",
    "wordsPositive = []\n",
    "\n",
    "for index, row in negX.iterrows():\n",
    "    sentence = row['title']\n",
    "    sentencesNegative.append(sentence)\n",
    "\n",
    "print(len(sentencesNegative))\n",
    "\n",
    "countVector = CountVectorizer()\n",
    "wordCountVector = countVector.fit_transform(sentencesNegative)\n",
    "\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(wordCountVector)\n",
    "\n",
    "matrix = countVector.transform(sentencesNegative)\n",
    "tf_idf_vector=tfidf_transformer.transform(matrix)\n",
    "\n",
    "feature_names = countVector.get_feature_names_out() \n",
    "first_document_vector=tf_idf_vector[0] \n",
    "\n",
    "df = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637bb671",
   "metadata": {},
   "outputs": [],
   "source": [
    "negDict = dict(zip(df.index, df['tfidf']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456fe8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF with vectorizer\n",
    "\n",
    "# settings that you use for count vectorizer will go here \n",
    "tfidf_vectorizer=TfidfVectorizer(use_idf=True) \n",
    "\n",
    "# just send in all your docs here \n",
    "tfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(sentencesNegative)\n",
    "# get the first vector out (for the first document) \n",
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0] \n",
    "\n",
    "# place tf-idf values in a pandas data frame \n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names_out(), columns=[\"tfidf\"]) \n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94c25964",
   "metadata": {},
   "source": [
    "We now have 2 dictionaries with word - TF-IDF pairings. TF-IDF scores range from 0 --> 1, where 1 is a heavy importance weighting and 0 is less important.\n",
    "\n",
    "Now that we have the top words for both positive and negative, we check for overlap with a threshold of .1 in the TF-IDF to see which words are important to a stock's drop and rise but also who's meanings depend on the surrounding words. Such words would be thrown into a model such as 'BERT' for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f8750",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsInBoth = []\n",
    "for key in posDict.keys():\n",
    "    if key in negDict.keys() and posDict[key] > 0.2 and negDict[key] > 0.1:\n",
    "        wordsInBoth.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wordsInBoth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only Therapeutics appears on both with a threshold of 0.1, which alludes to there being a 'clear' division between use of words and their related stock price changes.\n",
    "\n",
    "One major consideration is that each word comes from a different industry; one could extrapolate the causation when referring back to the original media and seeing what type of the 11 reports it was."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c309474",
   "metadata": {},
   "source": [
    "### Preliminary work\n",
    "\n",
    "- first cell identifies the most important word in each sentence.\n",
    "- second cell identifies a 'non' TF-IDF method of determining word frequency and most important words. It was determined not as powerful as the TF-IDF models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbda23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OTHER:\n",
    "# # Identify the most important word in each sentence\n",
    "# for sentence in cleanSentences:\n",
    "#     words = sentence.split()\n",
    "#     word_scores = {}\n",
    "#     for word in words:\n",
    "#         # Calculate word score as the product of its frequency and length\n",
    "#         score = len(word) * words.count(word)\n",
    "#         word_scores[word] = score\n",
    "#     mostImportantWordInSentence = max(word_scores, key=word_scores.get)\n",
    "#     importantWords.append(mostImportantWordInSentence)\n",
    "#     print(mostImportantWordInSentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e00039",
   "metadata": {},
   "outputs": [],
   "source": [
    "#most important words without TF-IDF\n",
    "# nltk.download('stopwords')\n",
    "# stopWords = set(stopwords.words('english'))\n",
    "\n",
    "# cleanSentences = []\n",
    "# importantWords = []\n",
    "# for sentence in sentencesPositive: #SWAP FOR SENTENCESNEGATIVE.\n",
    "#     words = sentence.split()\n",
    "#     filteredWords = [word for word in words if word.lower() not in stopWords]\n",
    "#     cleanSentence = ' '.join(filteredWords)\n",
    "#     cleanSentences.append(cleanSentence)\n",
    "\n",
    "\n",
    "# wordCounter = {}\n",
    "# for sentence in cleanSentences:\n",
    "#     words = sentence.split()\n",
    "#     word_scores = {}\n",
    "#     for word in words:\n",
    "#         # Calculate word score as the product of its frequency and length\n",
    "#         score = len(word) * words.count(word)\n",
    "#         word_scores[word] = score\n",
    "#     most_important_word = max(word_scores, key=word_scores.get)\n",
    "#     if most_important_word in wordCounter:\n",
    "#         wordCounter[most_important_word] += 1\n",
    "#     else:\n",
    "#         wordCounter[most_important_word] = 1\n",
    "\n",
    "# for sentence, count in wordCounter.items():\n",
    "#     print(f\"{sentence}: {count}\")\n",
    "\n",
    "\n",
    "# # Remove key-value pairings where the key contains the substring \".com\"\n",
    "# to_delete = []\n",
    "# for key in wordCounter.keys():\n",
    "#     if \".com\" in key or \"https:\" in key:\n",
    "#         to_delete.append(key)\n",
    "# for key in to_delete:\n",
    "#     del wordCounter[key]\n",
    "\n",
    "\n",
    "# top_words = sorted(wordCounter.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "\n",
    "# print(top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
